# Multimodal-DPO
Exploring DPO for improving reasoning in Vision Language Models

## Project Description
A implementation of Direct Preference Optimization (DPO) for vision-language models, specifically focusing on enhancing the reasoning capabilities of Google's PaliGemma model. The project uses preference data from the Sherlock dataset. The preference data is generated by using a model pool, and the code and data will be added soon. 

## TODO List
### Immediate Next Steps
- [ ] Add evaluation script for OOD evaluation
- [ ] Implement IPO (Identity Preference Optimization) loss variant
- [ ] Add example inference script with before/after comparisons

### Longer-term Goals
- [ ] Extend to other vision-language datasets (VCR, Visual Genome)
- [ ] Experiment with different reward modeling approaches
- [ ] Add support for KTO (Kahneman-Tversky Optimization) objective

